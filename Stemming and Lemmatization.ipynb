{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27a576b-a720-4672-8935-f488f62c0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tokenization is a fundamental concept in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens. These tokens can be individual words, subwords, or even characters. Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing. Let's consider an example to understand tokenization better. Imagine we have the following sentence: 'I love eating pizza.' When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza']. Here, each word is considered a separate token. However, the tokenization process can be more complex, especially for languages with compound words or morphological variations. Tokenization techniques can vary depending on the requirements of the task and the specific language being processed. For instance, some languages might employ subword tokenization, where words are broken down into smaller units. This can help capture morphological information and handle out-of-vocabulary words more effectively. In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements. For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens. Tokenization plays a crucial role in various NLP applications. For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text. In machine translation, tokens help align words and phrases across different languages. Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable. There are several libraries and tools available that offer tokenization functionalities for different programming languages. Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods. These libraries often come with pre-trained models that can handle tokenization for multiple languages. To practice tokenization, you can start by selecting a library and exploring its documentation and examples. Try tokenizing different sentences and texts, and observe the resulting tokens. Experiment with different tokenization options and consider the impact on downstream NLP tasks. Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand. By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis. Happy tokenizing!\" I hope this text provides you with ample material to practice tokenization. Let me know if you need any further assistance!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a60981-4012-4459-910f-69d85ef6ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db6ee1-ec57-485c-a81d-e039b4c6e621",
   "metadata": {},
   "source": [
    "<h2>Removing Punctuation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597a41ac-f25e-43f0-8014-885ed6100542",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization is a fundamental concept in natural language processing nlp it involves breaking down a piece of text such as a paragraph or a document into smaller units called tokens these tokens can be individual words subwords or even characters tokenization is an essential step in nlp tasks as it provides the foundation for further analysis and processing lets consider an example to understand tokenization better imagine we have the following sentence i love eating pizza when tokenized this sentence might be represented as i love eating pizza here each word is considered a separate token however the tokenization process can be more complex especially for languages with compound words or morphological variations tokenization techniques can vary depending on the requirements of the task and the specific language being processed for instance some languages might employ subword tokenization where words are broken down into smaller units this can help capture morphological information and handle outofvocabulary words more effectively in addition to breaking down text into tokens tokenization also involves handling punctuation special characters and other textual elements for example the sentence hello world might be tokenized into hello  world  where the commas and exclamation mark are treated as separate tokens tokenization plays a crucial role in various nlp applications for text classification tasks tokens serve as input features enabling the model to understand the semantic content of the text in machine translation tokens help align words and phrases across different languages sentiment analysis named entity recognition and information retrieval are other areas where tokenization proves valuable there are several libraries and tools available that offer tokenization functionalities for different programming languages pythonbased libraries like nltk spacy and the hugging face transformers library provide easytouse tokenization methods these libraries often come with pretrained models that can handle tokenization for multiple languages to practice tokenization you can start by selecting a library and exploring its documentation and examples try tokenizing different sentences and texts and observe the resulting tokens experiment with different tokenization options and consider the impact on downstream nlp tasks remember that tokenization is just the first step in nlp pipelines and subsequent processing steps like stemming lemmatization or stop word removal may be necessary depending on the task at hand by practicing tokenization on various texts you can gain a better understanding of how tokens are formed and how they contribute to nlp analysis happy tokenizing i hope this text provides you with ample material to practice tokenization let me know if you need any further assistance\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text_without_punctuation = text.translate(translator)\n",
    "text_without_punctuation = text_without_punctuation.lower()\n",
    "\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8d45d-40fb-46d7-a826-e3b0aec40be1",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fd44c-3c32-42a4-bc0a-ec5df955ba04",
   "metadata": {},
   "source": [
    "<h2>Leammatization using NLTK library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba107a51-06d4-41d2-a754-bed27178be30",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization ---> tokenization\n",
      "is ---> is\n",
      "a ---> a\n",
      "fundamental ---> fundamental\n",
      "concept ---> concept\n",
      "in ---> in\n",
      "natural ---> natural\n",
      "language ---> language\n",
      "processing ---> processing\n",
      "nlp ---> nlp\n",
      "it ---> it\n",
      "involves ---> involves\n",
      "breaking ---> breaking\n",
      "down ---> down\n",
      "a ---> a\n",
      "piece ---> piece\n",
      "of ---> of\n",
      "text ---> text\n",
      "such ---> such\n",
      "as ---> a\n",
      "a ---> a\n",
      "paragraph ---> paragraph\n",
      "or ---> or\n",
      "a ---> a\n",
      "document ---> document\n",
      "into ---> into\n",
      "smaller ---> smaller\n",
      "units ---> unit\n",
      "called ---> called\n",
      "tokens ---> token\n",
      "these ---> these\n",
      "tokens ---> token\n",
      "can ---> can\n",
      "be ---> be\n",
      "individual ---> individual\n",
      "words ---> word\n",
      "subwords ---> subwords\n",
      "or ---> or\n",
      "even ---> even\n",
      "characters ---> character\n",
      "tokenization ---> tokenization\n",
      "is ---> is\n",
      "an ---> an\n",
      "essential ---> essential\n",
      "step ---> step\n",
      "in ---> in\n",
      "nlp ---> nlp\n",
      "tasks ---> task\n",
      "as ---> a\n",
      "it ---> it\n",
      "provides ---> provides\n",
      "the ---> the\n",
      "foundation ---> foundation\n",
      "for ---> for\n",
      "further ---> further\n",
      "analysis ---> analysis\n",
      "and ---> and\n",
      "processing ---> processing\n",
      "lets ---> let\n",
      "consider ---> consider\n",
      "an ---> an\n",
      "example ---> example\n",
      "to ---> to\n",
      "understand ---> understand\n",
      "tokenization ---> tokenization\n",
      "better ---> better\n",
      "imagine ---> imagine\n",
      "we ---> we\n",
      "have ---> have\n",
      "the ---> the\n",
      "following ---> following\n",
      "sentence ---> sentence\n",
      "i ---> i\n",
      "love ---> love\n",
      "eating ---> eating\n",
      "pizza ---> pizza\n",
      "when ---> when\n",
      "tokenized ---> tokenized\n",
      "this ---> this\n",
      "sentence ---> sentence\n",
      "might ---> might\n",
      "be ---> be\n",
      "represented ---> represented\n",
      "as ---> a\n",
      "i ---> i\n",
      "love ---> love\n",
      "eating ---> eating\n",
      "pizza ---> pizza\n",
      "here ---> here\n",
      "each ---> each\n",
      "word ---> word\n",
      "is ---> is\n",
      "considered ---> considered\n",
      "a ---> a\n",
      "separate ---> separate\n",
      "token ---> token\n",
      "however ---> however\n",
      "the ---> the\n",
      "tokenization ---> tokenization\n",
      "process ---> process\n",
      "can ---> can\n",
      "be ---> be\n",
      "more ---> more\n",
      "complex ---> complex\n",
      "especially ---> especially\n",
      "for ---> for\n",
      "languages ---> language\n",
      "with ---> with\n",
      "compound ---> compound\n",
      "words ---> word\n",
      "or ---> or\n",
      "morphological ---> morphological\n",
      "variations ---> variation\n",
      "tokenization ---> tokenization\n",
      "techniques ---> technique\n",
      "can ---> can\n",
      "vary ---> vary\n",
      "depending ---> depending\n",
      "on ---> on\n",
      "the ---> the\n",
      "requirements ---> requirement\n",
      "of ---> of\n",
      "the ---> the\n",
      "task ---> task\n",
      "and ---> and\n",
      "the ---> the\n",
      "specific ---> specific\n",
      "language ---> language\n",
      "being ---> being\n",
      "processed ---> processed\n",
      "for ---> for\n",
      "instance ---> instance\n",
      "some ---> some\n",
      "languages ---> language\n",
      "might ---> might\n",
      "employ ---> employ\n",
      "subword ---> subword\n",
      "tokenization ---> tokenization\n",
      "where ---> where\n",
      "words ---> word\n",
      "are ---> are\n",
      "broken ---> broken\n",
      "down ---> down\n",
      "into ---> into\n",
      "smaller ---> smaller\n",
      "units ---> unit\n",
      "this ---> this\n",
      "can ---> can\n",
      "help ---> help\n",
      "capture ---> capture\n",
      "morphological ---> morphological\n",
      "information ---> information\n",
      "and ---> and\n",
      "handle ---> handle\n",
      "outofvocabulary ---> outofvocabulary\n",
      "words ---> word\n",
      "more ---> more\n",
      "effectively ---> effectively\n",
      "in ---> in\n",
      "addition ---> addition\n",
      "to ---> to\n",
      "breaking ---> breaking\n",
      "down ---> down\n",
      "text ---> text\n",
      "into ---> into\n",
      "tokens ---> token\n",
      "tokenization ---> tokenization\n",
      "also ---> also\n",
      "involves ---> involves\n",
      "handling ---> handling\n",
      "punctuation ---> punctuation\n",
      "special ---> special\n",
      "characters ---> character\n",
      "and ---> and\n",
      "other ---> other\n",
      "textual ---> textual\n",
      "elements ---> element\n",
      "for ---> for\n",
      "example ---> example\n",
      "the ---> the\n",
      "sentence ---> sentence\n",
      "hello ---> hello\n",
      "world ---> world\n",
      "might ---> might\n",
      "be ---> be\n",
      "tokenized ---> tokenized\n",
      "into ---> into\n",
      "hello ---> hello\n",
      "world ---> world\n",
      "where ---> where\n",
      "the ---> the\n",
      "commas ---> comma\n",
      "and ---> and\n",
      "exclamation ---> exclamation\n",
      "mark ---> mark\n",
      "are ---> are\n",
      "treated ---> treated\n",
      "as ---> a\n",
      "separate ---> separate\n",
      "tokens ---> token\n",
      "tokenization ---> tokenization\n",
      "plays ---> play\n",
      "a ---> a\n",
      "crucial ---> crucial\n",
      "role ---> role\n",
      "in ---> in\n",
      "various ---> various\n",
      "nlp ---> nlp\n",
      "applications ---> application\n",
      "for ---> for\n",
      "text ---> text\n",
      "classification ---> classification\n",
      "tasks ---> task\n",
      "tokens ---> token\n",
      "serve ---> serve\n",
      "as ---> a\n",
      "input ---> input\n",
      "features ---> feature\n",
      "enabling ---> enabling\n",
      "the ---> the\n",
      "model ---> model\n",
      "to ---> to\n",
      "understand ---> understand\n",
      "the ---> the\n",
      "semantic ---> semantic\n",
      "content ---> content\n",
      "of ---> of\n",
      "the ---> the\n",
      "text ---> text\n",
      "in ---> in\n",
      "machine ---> machine\n",
      "translation ---> translation\n",
      "tokens ---> token\n",
      "help ---> help\n",
      "align ---> align\n",
      "words ---> word\n",
      "and ---> and\n",
      "phrases ---> phrase\n",
      "across ---> across\n",
      "different ---> different\n",
      "languages ---> language\n",
      "sentiment ---> sentiment\n",
      "analysis ---> analysis\n",
      "named ---> named\n",
      "entity ---> entity\n",
      "recognition ---> recognition\n",
      "and ---> and\n",
      "information ---> information\n",
      "retrieval ---> retrieval\n",
      "are ---> are\n",
      "other ---> other\n",
      "areas ---> area\n",
      "where ---> where\n",
      "tokenization ---> tokenization\n",
      "proves ---> prof\n",
      "valuable ---> valuable\n",
      "there ---> there\n",
      "are ---> are\n",
      "several ---> several\n",
      "libraries ---> library\n",
      "and ---> and\n",
      "tools ---> tool\n",
      "available ---> available\n",
      "that ---> that\n",
      "offer ---> offer\n",
      "tokenization ---> tokenization\n",
      "functionalities ---> functionality\n",
      "for ---> for\n",
      "different ---> different\n",
      "programming ---> programming\n",
      "languages ---> language\n",
      "pythonbased ---> pythonbased\n",
      "libraries ---> library\n",
      "like ---> like\n",
      "nltk ---> nltk\n",
      "spacy ---> spacy\n",
      "and ---> and\n",
      "the ---> the\n",
      "hugging ---> hugging\n",
      "face ---> face\n",
      "transformers ---> transformer\n",
      "library ---> library\n",
      "provide ---> provide\n",
      "easytouse ---> easytouse\n",
      "tokenization ---> tokenization\n",
      "methods ---> method\n",
      "these ---> these\n",
      "libraries ---> library\n",
      "often ---> often\n",
      "come ---> come\n",
      "with ---> with\n",
      "pretrained ---> pretrained\n",
      "models ---> model\n",
      "that ---> that\n",
      "can ---> can\n",
      "handle ---> handle\n",
      "tokenization ---> tokenization\n",
      "for ---> for\n",
      "multiple ---> multiple\n",
      "languages ---> language\n",
      "to ---> to\n",
      "practice ---> practice\n",
      "tokenization ---> tokenization\n",
      "you ---> you\n",
      "can ---> can\n",
      "start ---> start\n",
      "by ---> by\n",
      "selecting ---> selecting\n",
      "a ---> a\n",
      "library ---> library\n",
      "and ---> and\n",
      "exploring ---> exploring\n",
      "its ---> it\n",
      "documentation ---> documentation\n",
      "and ---> and\n",
      "examples ---> example\n",
      "try ---> try\n",
      "tokenizing ---> tokenizing\n",
      "different ---> different\n",
      "sentences ---> sentence\n",
      "and ---> and\n",
      "texts ---> text\n",
      "and ---> and\n",
      "observe ---> observe\n",
      "the ---> the\n",
      "resulting ---> resulting\n",
      "tokens ---> token\n",
      "experiment ---> experiment\n",
      "with ---> with\n",
      "different ---> different\n",
      "tokenization ---> tokenization\n",
      "options ---> option\n",
      "and ---> and\n",
      "consider ---> consider\n",
      "the ---> the\n",
      "impact ---> impact\n",
      "on ---> on\n",
      "downstream ---> downstream\n",
      "nlp ---> nlp\n",
      "tasks ---> task\n",
      "remember ---> remember\n",
      "that ---> that\n",
      "tokenization ---> tokenization\n",
      "is ---> is\n",
      "just ---> just\n",
      "the ---> the\n",
      "first ---> first\n",
      "step ---> step\n",
      "in ---> in\n",
      "nlp ---> nlp\n",
      "pipelines ---> pipeline\n",
      "and ---> and\n",
      "subsequent ---> subsequent\n",
      "processing ---> processing\n",
      "steps ---> step\n",
      "like ---> like\n",
      "stemming ---> stemming\n",
      "lemmatization ---> lemmatization\n",
      "or ---> or\n",
      "stop ---> stop\n",
      "word ---> word\n",
      "removal ---> removal\n",
      "may ---> may\n",
      "be ---> be\n",
      "necessary ---> necessary\n",
      "depending ---> depending\n",
      "on ---> on\n",
      "the ---> the\n",
      "task ---> task\n",
      "at ---> at\n",
      "hand ---> hand\n",
      "by ---> by\n",
      "practicing ---> practicing\n",
      "tokenization ---> tokenization\n",
      "on ---> on\n",
      "various ---> various\n",
      "texts ---> text\n",
      "you ---> you\n",
      "can ---> can\n",
      "gain ---> gain\n",
      "a ---> a\n",
      "better ---> better\n",
      "understanding ---> understanding\n",
      "of ---> of\n",
      "how ---> how\n",
      "tokens ---> token\n",
      "are ---> are\n",
      "formed ---> formed\n",
      "and ---> and\n",
      "how ---> how\n",
      "they ---> they\n",
      "contribute ---> contribute\n",
      "to ---> to\n",
      "nlp ---> nlp\n",
      "analysis ---> analysis\n",
      "happy ---> happy\n",
      "tokenizing ---> tokenizing\n",
      "i ---> i\n",
      "hope ---> hope\n",
      "this ---> this\n",
      "text ---> text\n",
      "provides ---> provides\n",
      "you ---> you\n",
      "with ---> with\n",
      "ample ---> ample\n",
      "material ---> material\n",
      "to ---> to\n",
      "practice ---> practice\n",
      "tokenization ---> tokenization\n",
      "let ---> let\n",
      "me ---> me\n",
      "know ---> know\n",
      "if ---> if\n",
      "you ---> you\n",
      "need ---> need\n",
      "any ---> any\n",
      "further ---> further\n",
      "assistance ---> assistance\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = nltk.word_tokenize(text_without_punctuation)\n",
    "\n",
    "for words in tokens:\n",
    "    print(words + \" ---> \" + lemmatizer.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94d67e-5dff-4ef3-b613-180b952094e6",
   "metadata": {},
   "source": [
    "<h2>Leammatization using SpaCy library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f4afd4-e52b-411d-8c73-613ed7c67467",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization ---> tokenization\n",
      "is ---> be\n",
      "a ---> a\n",
      "fundamental ---> fundamental\n",
      "concept ---> concept\n",
      "in ---> in\n",
      "natural ---> natural\n",
      "language ---> language\n",
      "processing ---> processing\n",
      "nlp ---> nlp\n",
      "it ---> it\n",
      "involves ---> involve\n",
      "breaking ---> break\n",
      "down ---> down\n",
      "a ---> a\n",
      "piece ---> piece\n",
      "of ---> of\n",
      "text ---> text\n",
      "such ---> such\n",
      "as ---> as\n",
      "a ---> a\n",
      "paragraph ---> paragraph\n",
      "or ---> or\n",
      "a ---> a\n",
      "document ---> document\n",
      "into ---> into\n",
      "smaller ---> small\n",
      "units ---> unit\n",
      "called ---> call\n",
      "tokens ---> token\n",
      "these ---> these\n",
      "tokens ---> token\n",
      "can ---> can\n",
      "be ---> be\n",
      "individual ---> individual\n",
      "words ---> word\n",
      "subwords ---> subword\n",
      "or ---> or\n",
      "even ---> even\n",
      "characters ---> character\n",
      "tokenization ---> tokenization\n",
      "is ---> be\n",
      "an ---> an\n",
      "essential ---> essential\n",
      "step ---> step\n",
      "in ---> in\n",
      "nlp ---> nlp\n",
      "tasks ---> task\n",
      "as ---> as\n",
      "it ---> it\n",
      "provides ---> provide\n",
      "the ---> the\n",
      "foundation ---> foundation\n",
      "for ---> for\n",
      "further ---> further\n",
      "analysis ---> analysis\n",
      "and ---> and\n",
      "processing ---> processing\n",
      "lets ---> let\n",
      "consider ---> consider\n",
      "an ---> an\n",
      "example ---> example\n",
      "to ---> to\n",
      "understand ---> understand\n",
      "tokenization ---> tokenization\n",
      "better ---> well\n",
      "imagine ---> imagine\n",
      "we ---> we\n",
      "have ---> have\n",
      "the ---> the\n",
      "following ---> following\n",
      "sentence ---> sentence\n",
      "i ---> I\n",
      "love ---> love\n",
      "eating ---> eat\n",
      "pizza ---> pizza\n",
      "when ---> when\n",
      "tokenized ---> tokenize\n",
      "this ---> this\n",
      "sentence ---> sentence\n",
      "might ---> might\n",
      "be ---> be\n",
      "represented ---> represent\n",
      "as ---> as\n",
      "i ---> I\n",
      "love ---> love\n",
      "eating ---> eat\n",
      "pizza ---> pizza\n",
      "here ---> here\n",
      "each ---> each\n",
      "word ---> word\n",
      "is ---> be\n",
      "considered ---> consider\n",
      "a ---> a\n",
      "separate ---> separate\n",
      "token ---> token\n",
      "however ---> however\n",
      "the ---> the\n",
      "tokenization ---> tokenization\n",
      "process ---> process\n",
      "can ---> can\n",
      "be ---> be\n",
      "more ---> more\n",
      "complex ---> complex\n",
      "especially ---> especially\n",
      "for ---> for\n",
      "languages ---> language\n",
      "with ---> with\n",
      "compound ---> compound\n",
      "words ---> word\n",
      "or ---> or\n",
      "morphological ---> morphological\n",
      "variations ---> variation\n",
      "tokenization ---> tokenization\n",
      "techniques ---> technique\n",
      "can ---> can\n",
      "vary ---> vary\n",
      "depending ---> depend\n",
      "on ---> on\n",
      "the ---> the\n",
      "requirements ---> requirement\n",
      "of ---> of\n",
      "the ---> the\n",
      "task ---> task\n",
      "and ---> and\n",
      "the ---> the\n",
      "specific ---> specific\n",
      "language ---> language\n",
      "being ---> be\n",
      "processed ---> process\n",
      "for ---> for\n",
      "instance ---> instance\n",
      "some ---> some\n",
      "languages ---> language\n",
      "might ---> might\n",
      "employ ---> employ\n",
      "subword ---> subword\n",
      "tokenization ---> tokenization\n",
      "where ---> where\n",
      "words ---> word\n",
      "are ---> be\n",
      "broken ---> break\n",
      "down ---> down\n",
      "into ---> into\n",
      "smaller ---> small\n",
      "units ---> unit\n",
      "this ---> this\n",
      "can ---> can\n",
      "help ---> help\n",
      "capture ---> capture\n",
      "morphological ---> morphological\n",
      "information ---> information\n",
      "and ---> and\n",
      "handle ---> handle\n",
      "outofvocabulary ---> outofvocabulary\n",
      "words ---> word\n",
      "more ---> more\n",
      "effectively ---> effectively\n",
      "in ---> in\n",
      "addition ---> addition\n",
      "to ---> to\n",
      "breaking ---> break\n",
      "down ---> down\n",
      "text ---> text\n",
      "into ---> into\n",
      "tokens ---> tokens\n",
      "tokenization ---> tokenization\n",
      "also ---> also\n",
      "involves ---> involve\n",
      "handling ---> handle\n",
      "punctuation ---> punctuation\n",
      "special ---> special\n",
      "characters ---> character\n",
      "and ---> and\n",
      "other ---> other\n",
      "textual ---> textual\n",
      "elements ---> element\n",
      "for ---> for\n",
      "example ---> example\n",
      "the ---> the\n",
      "sentence ---> sentence\n",
      "hello ---> hello\n",
      "world ---> world\n",
      "might ---> might\n",
      "be ---> be\n",
      "tokenized ---> tokenize\n",
      "into ---> into\n",
      "hello ---> hello\n",
      "  --->  \n",
      "world ---> world\n",
      "  --->  \n",
      "where ---> where\n",
      "the ---> the\n",
      "commas ---> commas\n",
      "and ---> and\n",
      "exclamation ---> exclamation\n",
      "mark ---> mark\n",
      "are ---> be\n",
      "treated ---> treat\n",
      "as ---> as\n",
      "separate ---> separate\n",
      "tokens ---> token\n",
      "tokenization ---> tokenization\n",
      "plays ---> play\n",
      "a ---> a\n",
      "crucial ---> crucial\n",
      "role ---> role\n",
      "in ---> in\n",
      "various ---> various\n",
      "nlp ---> nlp\n",
      "applications ---> application\n",
      "for ---> for\n",
      "text ---> text\n",
      "classification ---> classification\n",
      "tasks ---> task\n",
      "tokens ---> token\n",
      "serve ---> serve\n",
      "as ---> as\n",
      "input ---> input\n",
      "features ---> feature\n",
      "enabling ---> enable\n",
      "the ---> the\n",
      "model ---> model\n",
      "to ---> to\n",
      "understand ---> understand\n",
      "the ---> the\n",
      "semantic ---> semantic\n",
      "content ---> content\n",
      "of ---> of\n",
      "the ---> the\n",
      "text ---> text\n",
      "in ---> in\n",
      "machine ---> machine\n",
      "translation ---> translation\n",
      "tokens ---> token\n",
      "help ---> help\n",
      "align ---> align\n",
      "words ---> word\n",
      "and ---> and\n",
      "phrases ---> phrase\n",
      "across ---> across\n",
      "different ---> different\n",
      "languages ---> language\n",
      "sentiment ---> sentiment\n",
      "analysis ---> analysis\n",
      "named ---> name\n",
      "entity ---> entity\n",
      "recognition ---> recognition\n",
      "and ---> and\n",
      "information ---> information\n",
      "retrieval ---> retrieval\n",
      "are ---> be\n",
      "other ---> other\n",
      "areas ---> area\n",
      "where ---> where\n",
      "tokenization ---> tokenization\n",
      "proves ---> prove\n",
      "valuable ---> valuable\n",
      "there ---> there\n",
      "are ---> be\n",
      "several ---> several\n",
      "libraries ---> library\n",
      "and ---> and\n",
      "tools ---> tool\n",
      "available ---> available\n",
      "that ---> that\n",
      "offer ---> offer\n",
      "tokenization ---> tokenization\n",
      "functionalities ---> functionality\n",
      "for ---> for\n",
      "different ---> different\n",
      "programming ---> programming\n",
      "languages ---> language\n",
      "pythonbased ---> pythonbase\n",
      "libraries ---> library\n",
      "like ---> like\n",
      "nltk ---> nltk\n",
      "spacy ---> spacy\n",
      "and ---> and\n",
      "the ---> the\n",
      "hugging ---> hug\n",
      "face ---> face\n",
      "transformers ---> transformer\n",
      "library ---> library\n",
      "provide ---> provide\n",
      "easytouse ---> easytouse\n",
      "tokenization ---> tokenization\n",
      "methods ---> method\n",
      "these ---> these\n",
      "libraries ---> library\n",
      "often ---> often\n",
      "come ---> come\n",
      "with ---> with\n",
      "pretrained ---> pretraine\n",
      "models ---> model\n",
      "that ---> that\n",
      "can ---> can\n",
      "handle ---> handle\n",
      "tokenization ---> tokenization\n",
      "for ---> for\n",
      "multiple ---> multiple\n",
      "languages ---> language\n",
      "to ---> to\n",
      "practice ---> practice\n",
      "tokenization ---> tokenization\n",
      "you ---> you\n",
      "can ---> can\n",
      "start ---> start\n",
      "by ---> by\n",
      "selecting ---> select\n",
      "a ---> a\n",
      "library ---> library\n",
      "and ---> and\n",
      "exploring ---> explore\n",
      "its ---> its\n",
      "documentation ---> documentation\n",
      "and ---> and\n",
      "examples ---> example\n",
      "try ---> try\n",
      "tokenizing ---> tokenize\n",
      "different ---> different\n",
      "sentences ---> sentence\n",
      "and ---> and\n",
      "texts ---> text\n",
      "and ---> and\n",
      "observe ---> observe\n",
      "the ---> the\n",
      "resulting ---> result\n",
      "tokens ---> token\n",
      "experiment ---> experiment\n",
      "with ---> with\n",
      "different ---> different\n",
      "tokenization ---> tokenization\n",
      "options ---> option\n",
      "and ---> and\n",
      "consider ---> consider\n",
      "the ---> the\n",
      "impact ---> impact\n",
      "on ---> on\n",
      "downstream ---> downstream\n",
      "nlp ---> nlp\n",
      "tasks ---> task\n",
      "remember ---> remember\n",
      "that ---> that\n",
      "tokenization ---> tokenization\n",
      "is ---> be\n",
      "just ---> just\n",
      "the ---> the\n",
      "first ---> first\n",
      "step ---> step\n",
      "in ---> in\n",
      "nlp ---> nlp\n",
      "pipelines ---> pipeline\n",
      "and ---> and\n",
      "subsequent ---> subsequent\n",
      "processing ---> processing\n",
      "steps ---> step\n",
      "like ---> like\n",
      "stemming ---> stem\n",
      "lemmatization ---> lemmatization\n",
      "or ---> or\n",
      "stop ---> stop\n",
      "word ---> word\n",
      "removal ---> removal\n",
      "may ---> may\n",
      "be ---> be\n",
      "necessary ---> necessary\n",
      "depending ---> depend\n",
      "on ---> on\n",
      "the ---> the\n",
      "task ---> task\n",
      "at ---> at\n",
      "hand ---> hand\n",
      "by ---> by\n",
      "practicing ---> practice\n",
      "tokenization ---> tokenization\n",
      "on ---> on\n",
      "various ---> various\n",
      "texts ---> text\n",
      "you ---> you\n",
      "can ---> can\n",
      "gain ---> gain\n",
      "a ---> a\n",
      "better ---> well\n",
      "understanding ---> understanding\n",
      "of ---> of\n",
      "how ---> how\n",
      "tokens ---> token\n",
      "are ---> be\n",
      "formed ---> form\n",
      "and ---> and\n",
      "how ---> how\n",
      "they ---> they\n",
      "contribute ---> contribute\n",
      "to ---> to\n",
      "nlp ---> nlp\n",
      "analysis ---> analysis\n",
      "happy ---> happy\n",
      "tokenizing ---> tokenizing\n",
      "i ---> I\n",
      "hope ---> hope\n",
      "this ---> this\n",
      "text ---> text\n",
      "provides ---> provide\n",
      "you ---> you\n",
      "with ---> with\n",
      "ample ---> ample\n",
      "material ---> material\n",
      "to ---> to\n",
      "practice ---> practice\n",
      "tokenization ---> tokenization\n",
      "let ---> let\n",
      "me ---> I\n",
      "know ---> know\n",
      "if ---> if\n",
      "you ---> you\n",
      "need ---> need\n",
      "any ---> any\n",
      "further ---> further\n",
      "assistance ---> assistance\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text_without_punctuation)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + \" ---> \" + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db66f7-5e75-474c-a53d-2fa1617678e0",
   "metadata": {},
   "source": [
    "<h2>Lemmatization using NLTK and POS tagging</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff21a49d-2cea-4d2c-b998-59afa3c15676",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization be a fundamental concept in natural language processing nlp it involve break down a piece of text such as a paragraph or a document into small unit call token these token can be individual word subwords or even character tokenization be an essential step in nlp task as it provide the foundation for further analysis and processing let consider an example to understand tokenization well imagine we have the following sentence i love eat pizza when tokenized this sentence might be represent as i love eat pizza here each word be consider a separate token however the tokenization process can be more complex especially for language with compound word or morphological variation tokenization technique can vary depend on the requirement of the task and the specific language be process for instance some language might employ subword tokenization where word be break down into small unit this can help capture morphological information and handle outofvocabulary word more effectively in addition to break down text into token tokenization also involve handle punctuation special character and other textual element for example the sentence hello world might be tokenized into hello world where the comma and exclamation mark be treat as separate token tokenization play a crucial role in various nlp application for text classification task tokens serve as input feature enable the model to understand the semantic content of the text in machine translation tokens help align word and phrase across different language sentiment analysis name entity recognition and information retrieval be other area where tokenization prove valuable there be several library and tool available that offer tokenization functionality for different programming language pythonbased library like nltk spacy and the hugging face transformer library provide easytouse tokenization method these library often come with pretrained model that can handle tokenization for multiple language to practice tokenization you can start by select a library and explore its documentation and example try tokenizing different sentence and text and observe the result token experiment with different tokenization option and consider the impact on downstream nlp task remember that tokenization be just the first step in nlp pipeline and subsequent processing step like stem lemmatization or stop word removal may be necessary depend on the task at hand by practice tokenization on various texts you can gain a good understanding of how token be form and how they contribute to nlp analysis happy tokenizing i hope this text provide you with ample material to practice tokenization let me know if you need any further assistance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abhik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "text = \"\"\"Tokenization is a fundamental concept in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens. These tokens can be individual words, subwords, or even characters. Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing. Let's consider an example to understand tokenization better. Imagine we have the following sentence: 'I love eating pizza.' When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza']. Here, each word is considered a separate token. However, the tokenization process can be more complex, especially for languages with compound words or morphological variations. Tokenization techniques can vary depending on the requirements of the task and the specific language being processed. For instance, some languages might employ subword tokenization, where words are broken down into smaller units. This can help capture morphological information and handle out-of-vocabulary words more effectively. In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements. For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens. Tokenization plays a crucial role in various NLP applications. For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text. In machine translation, tokens help align words and phrases across different languages. Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable. There are several libraries and tools available that offer tokenization functionalities for different programming languages. Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods. These libraries often come with pre-trained models that can handle tokenization for multiple languages. To practice tokenization, you can start by selecting a library and exploring its documentation and examples. Try tokenizing different sentences and texts, and observe the resulting tokens. Experiment with different tokenization options and consider the impact on downstream NLP tasks. Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand. By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis. Happy tokenizing!\" I hope this text provides you with ample material to practice tokenization. Let me know if you need any further assistance!\"\"\"\n",
    "\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(text_without_punctuation))\n",
    "\n",
    "wordnet_tagged = [(word, pos_tagger(tag)) for word, tag in pos_tagged]\n",
    "\n",
    "lemmatized_sentence = []\n",
    "\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94120f53-b5cb-4b73-a1e8-68994330dd63",
   "metadata": {},
   "source": [
    "<h1>Stemming</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7330d-fd0e-4658-a638-b2b83e410226",
   "metadata": {},
   "source": [
    "<h2>Stemming using NLTK library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9765681b-6001-4bfa-a67c-917b9f17aceb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token is a fundament concept in natur languag process nlp it involv break down a piec of text such as a paragraph or a document into smaller unit call token these token can be individu word subword or even charact token is an essenti step in nlp task as it provid the foundat for further analysi and process let consid an exampl to understand token better imagin we have the follow sentenc i love eat pizza when token thi sentenc might be repres as i love eat pizza here each word is consid a separ token howev the token process can be more complex especi for languag with compound word or morpholog variat token techniqu can vari depend on the requir of the task and the specif languag be process for instanc some languag might employ subword token where word are broken down into smaller unit thi can help captur morpholog inform and handl outofvocabulari word more effect in addit to break down text into token token also involv handl punctuat special charact and other textual element for exampl the sentenc hello world might be token into hello world where the comma and exclam mark are treat as separ token token play a crucial role in variou nlp applic for text classif task token serv as input featur enabl the model to understand the semant content of the text in machin translat token help align word and phrase across differ languag sentiment analysi name entiti recognit and inform retriev are other area where token prove valuabl there are sever librari and tool avail that offer token function for differ program languag pythonbas librari like nltk spaci and the hug face transform librari provid easytous token method these librari often come with pretrain model that can handl token for multipl languag to practic token you can start by select a librari and explor it document and exampl tri token differ sentenc and text and observ the result token experi with differ token option and consid the impact on downstream nlp task rememb that token is just the first step in nlp pipelin and subsequ process step like stem lemmat or stop word remov may be necessari depend on the task at hand by practic token on variou text you can gain a better understand of how token are form and how they contribut to nlp analysi happi token i hope thi text provid you with ampl materi to practic token let me know if you need ani further assist\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text_without_punctuation)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "for token in tokens:\n",
    "    stemmed_word = stemmer.stem(token)\n",
    "    stemmed_words.append(stemmed_word)\n",
    "    \n",
    "stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ab89f-9198-4a6a-85f0-5c1973e8e888",
   "metadata": {},
   "source": [
    "<h2>Stemming using SpaCy library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1891b05d-e0e7-4a19-a695-9524d26bbe61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization be a fundamental concept in natural language processing nlp it involve break down a piece of text such as a paragraph or a document into small unit call token these token can be individual word subword or even character tokenization be an essential step in nlp task as it provide the foundation for further analysis and processing let consider an example to understand tokenization well imagine we have the following sentence I love eat pizza when tokenize this sentence might be represent as I love eat pizza here each word be consider a separate token however the tokenization process can be more complex especially for language with compound word or morphological variation tokenization technique can vary depend on the requirement of the task and the specific language be process for instance some language might employ subword tokenization where word be break down into small unit this can help capture morphological information and handle outofvocabulary word more effectively in addition to break down text into tokens tokenization also involve handle punctuation special character and other textual element for example the sentence hello world might be tokenize into hello   world   where the commas and exclamation mark be treat as separate token tokenization play a crucial role in various nlp application for text classification task token serve as input feature enable the model to understand the semantic content of the text in machine translation token help align word and phrase across different language sentiment analysis name entity recognition and information retrieval be other area where tokenization prove valuable there be several library and tool available that offer tokenization functionality for different programming language pythonbase library like nltk spacy and the hug face transformer library provide easytouse tokenization method these library often come with pretraine model that can handle tokenization for multiple language to practice tokenization you can start by select a library and explore its documentation and example try tokenize different sentence and text and observe the result token experiment with different tokenization option and consider the impact on downstream nlp task remember that tokenization be just the first step in nlp pipeline and subsequent processing step like stem lemmatization or stop word removal may be necessary depend on the task at hand by practice tokenization on various text you can gain a well understanding of how token be form and how they contribute to nlp analysis happy tokenizing I hope this text provide you with ample material to practice tokenization let I know if you need any further assistance\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def stem_token(token):\n",
    "    stem = token.lemma_\n",
    "    return stem\n",
    "\n",
    "spacy.tokens.Token.set_extension(\"stem\", getter=stem_token)\n",
    "\n",
    "doc = nlp(text_without_punctuation)\n",
    "\n",
    "stemmed_words = [token._.stem for token in doc]\n",
    "\n",
    "stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88efbe7-505b-4fef-b667-e6c2af7390a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
