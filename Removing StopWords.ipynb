{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c1404e-8a07-4cb2-84d4-b8c27353b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tokenization is a fundamental concept in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens. These tokens can be individual words, subwords, or even characters. Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing. Let's consider an example to understand tokenization better. Imagine we have the following sentence: 'I love eating pizza.' When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza']. Here, each word is considered a separate token. However, the tokenization process can be more complex, especially for languages with compound words or morphological variations. Tokenization techniques can vary depending on the requirements of the task and the specific language being processed. For instance, some languages might employ subword tokenization, where words are broken down into smaller units. This can help capture morphological information and handle out-of-vocabulary words more effectively. In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements. For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens. Tokenization plays a crucial role in various NLP applications. For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text. In machine translation, tokens help align words and phrases across different languages. Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable. There are several libraries and tools available that offer tokenization functionalities for different programming languages. Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods. These libraries often come with pre-trained models that can handle tokenization for multiple languages. To practice tokenization, you can start by selecting a library and exploring its documentation and examples. Try tokenizing different sentences and texts, and observe the resulting tokens. Experiment with different tokenization options and consider the impact on downstream NLP tasks. Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand. By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis. Happy tokenizing!\" I hope this text provides you with ample material to practice tokenization. Let me know if you need any further assistance!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67989608-f743-4447-97d6-f67b6d171689",
   "metadata": {},
   "source": [
    "<h1>Filtering out Stopwords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e18272-6311-4029-945d-a240e7d56832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e2ef4-ad4a-4cc7-a02c-e26ba48f8054",
   "metadata": {},
   "source": [
    "<h2>Using NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976dc962-28eb-43bf-8b87-6a854d868ebf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhik\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization fundamental concept Natural Language Processing ( NLP ) . involves breaking piece text , paragraph document , smaller units called tokens . tokens individual words , subwords , even characters . Tokenization essential step NLP tasks provides foundation analysis processing . Let 's consider example understand tokenization better . Imagine following sentence : ' love eating pizza . ' tokenized , sentence might represented [ ' ' , 'love ' , 'eating ' , 'pizza ' ] . , word considered separate token . However , tokenization process complex , especially languages compound words morphological variations . Tokenization techniques vary depending requirements task specific language processed . instance , languages might employ subword tokenization , words broken smaller units . help capture morphological information handle out-of-vocabulary words effectively . addition breaking text tokens , tokenization also involves handling punctuation , special characters , textual elements . example , sentence 'Hello , world ! ' might tokenized [ 'Hello ' , ' , ' , 'world ' , ' ! ' ] , commas exclamation mark treated separate tokens . Tokenization plays crucial role various NLP applications . text classification tasks , tokens serve input features , enabling model understand semantic content text . machine translation , tokens help align words phrases across different languages . Sentiment analysis , named entity recognition , information retrieval areas tokenization proves valuable . several libraries tools available offer tokenization functionalities different programming languages . Python-based libraries like NLTK , spaCy , Hugging Face Transformers library provide easy-to-use tokenization methods . libraries often come pre-trained models handle tokenization multiple languages . practice tokenization , start selecting library exploring documentation examples . Try tokenizing different sentences texts , observe resulting tokens . Experiment different tokenization options consider impact downstream NLP tasks . Remember tokenization first step NLP pipelines , subsequent processing steps like stemming , lemmatization , stop word removal may necessary depending task hand . practicing tokenization various texts , gain better understanding tokens formed contribute NLP analysis . Happy tokenizing ! '' hope text provides ample material practice tokenization . Let know need assistance !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70563889-8d00-4849-8947-bdd3d30672d6",
   "metadata": {},
   "source": [
    "<h2>Using SpaCy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1710d0ce-e71a-4276-a240-8cc60c5270ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization fundamental concept Natural Language Processing ( NLP ) . involves breaking piece text , paragraph document , smaller units called tokens . tokens individual words , subwords , characters . Tokenization essential step NLP tasks provides foundation analysis processing . Let consider example understand tokenization better . Imagine following sentence : ' love eating pizza . ' tokenized , sentence represented [ ' ' , ' love ' , ' eating ' , ' pizza ' ] . , word considered separate token . , tokenization process complex , especially languages compound words morphological variations . Tokenization techniques vary depending requirements task specific language processed . instance , languages employ subword tokenization , words broken smaller units . help capture morphological information handle - - vocabulary words effectively . addition breaking text tokens , tokenization involves handling punctuation , special characters , textual elements . example , sentence ' Hello , world ! ' tokenized [ ' Hello ' , ' , ' , ' world ' , ' ! ' ] , commas exclamation mark treated separate tokens . Tokenization plays crucial role NLP applications . text classification tasks , tokens serve input features , enabling model understand semantic content text . machine translation , tokens help align words phrases different languages . Sentiment analysis , named entity recognition , information retrieval areas tokenization proves valuable . libraries tools available offer tokenization functionalities different programming languages . Python - based libraries like NLTK , spaCy , Hugging Face Transformers library provide easy - - use tokenization methods . libraries come pre - trained models handle tokenization multiple languages . practice tokenization , start selecting library exploring documentation examples . Try tokenizing different sentences texts , observe resulting tokens . Experiment different tokenization options consider impact downstream NLP tasks . Remember tokenization step NLP pipelines , subsequent processing steps like stemming , lemmatization , stop word removal necessary depending task hand . practicing tokenization texts , gain better understanding tokens formed contribute NLP analysis . Happy tokenizing ! \" hope text provides ample material practice tokenization . Let know need assistance !\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "print(filtered_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
