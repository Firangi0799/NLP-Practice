{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2216c2e4-a28a-4f88-b371-516ae6e3919c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Tokenization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9f48a6-f004-493a-abc7-2ba9e5829286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5942faf-ae93-42cc-91d3-046245282fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Tokenization is a fundamental concept in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens. These tokens can be individual words, subwords, or even characters. Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing. Let's consider an example to understand tokenization better. Imagine we have the following sentence: 'I love eating pizza.' When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza']. Here, each word is considered a separate token. However, the tokenization process can be more complex, especially for languages with compound words or morphological variations. Tokenization techniques can vary depending on the requirements of the task and the specific language being processed. For instance, some languages might employ subword tokenization, where words are broken down into smaller units. This can help capture morphological information and handle out-of-vocabulary words more effectively. In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements. For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens. Tokenization plays a crucial role in various NLP applications. For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text. In machine translation, tokens help align words and phrases across different languages. Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable. There are several libraries and tools available that offer tokenization functionalities for different programming languages. Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods. These libraries often come with pre-trained models that can handle tokenization for multiple languages. To practice tokenization, you can start by selecting a library and exploring its documentation and examples. Try tokenizing different sentences and texts, and observe the resulting tokens. Experiment with different tokenization options and consider the impact on downstream NLP tasks. Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand. By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis. Happy tokenizing!\" I hope this text provides you with ample material to practice tokenization. Let me know if you need any further assistance!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5308db-d418-4b9e-bceb-b821efeba353",
   "metadata": {},
   "source": [
    "<h2>Removing Punctuation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "360ed269-5629-46a1-b4ea-8ca85a9a1196",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization is a fundamental concept in natural language processing nlp it involves breaking down a piece of text such as a paragraph or a document into smaller units called tokens these tokens can be individual words subwords or even characters tokenization is an essential step in nlp tasks as it provides the foundation for further analysis and processing lets consider an example to understand tokenization better imagine we have the following sentence i love eating pizza when tokenized this sentence might be represented as i love eating pizza here each word is considered a separate token however the tokenization process can be more complex especially for languages with compound words or morphological variations tokenization techniques can vary depending on the requirements of the task and the specific language being processed for instance some languages might employ subword tokenization where words are broken down into smaller units this can help capture morphological information and handle outofvocabulary words more effectively in addition to breaking down text into tokens tokenization also involves handling punctuation special characters and other textual elements for example the sentence hello world might be tokenized into hello  world  where the commas and exclamation mark are treated as separate tokens tokenization plays a crucial role in various nlp applications for text classification tasks tokens serve as input features enabling the model to understand the semantic content of the text in machine translation tokens help align words and phrases across different languages sentiment analysis named entity recognition and information retrieval are other areas where tokenization proves valuable there are several libraries and tools available that offer tokenization functionalities for different programming languages pythonbased libraries like nltk spacy and the hugging face transformers library provide easytouse tokenization methods these libraries often come with pretrained models that can handle tokenization for multiple languages to practice tokenization you can start by selecting a library and exploring its documentation and examples try tokenizing different sentences and texts and observe the resulting tokens experiment with different tokenization options and consider the impact on downstream nlp tasks remember that tokenization is just the first step in nlp pipelines and subsequent processing steps like stemming lemmatization or stop word removal may be necessary depending on the task at hand by practicing tokenization on various texts you can gain a better understanding of how tokens are formed and how they contribute to nlp analysis happy tokenizing i hope this text provides you with ample material to practice tokenization let me know if you need any further assistance\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text_without_punctuation = text.translate(translator)\n",
    "text_without_punctuation = text_without_punctuation.lower()\n",
    "\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c1c2d2c-4c30-4834-8c89-39be06855b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5abcd0-ae0a-46fc-82b6-096869e8ea31",
   "metadata": {},
   "source": [
    "<h2>Sentence tokenization and word tokenization using NLTK library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f67a5b4-9450-4214-8919-0f8bbd274051",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Sentence Tokenization:\n",
      "['Tokenization is a fundamental concept in Natural Language Processing (NLP).', 'It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens.', 'These tokens can be individual words, subwords, or even characters.', 'Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing.', \"Let's consider an example to understand tokenization better.\", \"Imagine we have the following sentence: 'I love eating pizza.'\", \"When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza'].\", 'Here, each word is considered a separate token.', 'However, the tokenization process can be more complex, especially for languages with compound words or morphological variations.', 'Tokenization techniques can vary depending on the requirements of the task and the specific language being processed.', 'For instance, some languages might employ subword tokenization, where words are broken down into smaller units.', 'This can help capture morphological information and handle out-of-vocabulary words more effectively.', 'In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements.', \"For example, the sentence 'Hello, world!'\", \"might be tokenized into ['Hello', ',', 'world', '!\", \"'], where the commas and exclamation mark are treated as separate tokens.\", 'Tokenization plays a crucial role in various NLP applications.', 'For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text.', 'In machine translation, tokens help align words and phrases across different languages.', 'Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable.', 'There are several libraries and tools available that offer tokenization functionalities for different programming languages.', 'Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods.', 'These libraries often come with pre-trained models that can handle tokenization for multiple languages.', 'To practice tokenization, you can start by selecting a library and exploring its documentation and examples.', 'Try tokenizing different sentences and texts, and observe the resulting tokens.', 'Experiment with different tokenization options and consider the impact on downstream NLP tasks.', 'Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand.', 'By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis.', 'Happy tokenizing!\"', 'I hope this text provides you with ample material to practice tokenization.', 'Let me know if you need any further assistance!']\n",
      "\n",
      "NLTK Word Tokenization:\n",
      "['tokenization', 'is', 'a', 'fundamental', 'concept', 'in', 'natural', 'language', 'processing', 'nlp', 'it', 'involves', 'breaking', 'down', 'a', 'piece', 'of', 'text', 'such', 'as', 'a', 'paragraph', 'or', 'a', 'document', 'into', 'smaller', 'units', 'called', 'tokens', 'these', 'tokens', 'can', 'be', 'individual', 'words', 'subwords', 'or', 'even', 'characters', 'tokenization', 'is', 'an', 'essential', 'step', 'in', 'nlp', 'tasks', 'as', 'it', 'provides', 'the', 'foundation', 'for', 'further', 'analysis', 'and', 'processing', 'lets', 'consider', 'an', 'example', 'to', 'understand', 'tokenization', 'better', 'imagine', 'we', 'have', 'the', 'following', 'sentence', 'i', 'love', 'eating', 'pizza', 'when', 'tokenized', 'this', 'sentence', 'might', 'be', 'represented', 'as', 'i', 'love', 'eating', 'pizza', 'here', 'each', 'word', 'is', 'considered', 'a', 'separate', 'token', 'however', 'the', 'tokenization', 'process', 'can', 'be', 'more', 'complex', 'especially', 'for', 'languages', 'with', 'compound', 'words', 'or', 'morphological', 'variations', 'tokenization', 'techniques', 'can', 'vary', 'depending', 'on', 'the', 'requirements', 'of', 'the', 'task', 'and', 'the', 'specific', 'language', 'being', 'processed', 'for', 'instance', 'some', 'languages', 'might', 'employ', 'subword', 'tokenization', 'where', 'words', 'are', 'broken', 'down', 'into', 'smaller', 'units', 'this', 'can', 'help', 'capture', 'morphological', 'information', 'and', 'handle', 'outofvocabulary', 'words', 'more', 'effectively', 'in', 'addition', 'to', 'breaking', 'down', 'text', 'into', 'tokens', 'tokenization', 'also', 'involves', 'handling', 'punctuation', 'special', 'characters', 'and', 'other', 'textual', 'elements', 'for', 'example', 'the', 'sentence', 'hello', 'world', 'might', 'be', 'tokenized', 'into', 'hello', 'world', 'where', 'the', 'commas', 'and', 'exclamation', 'mark', 'are', 'treated', 'as', 'separate', 'tokens', 'tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'various', 'nlp', 'applications', 'for', 'text', 'classification', 'tasks', 'tokens', 'serve', 'as', 'input', 'features', 'enabling', 'the', 'model', 'to', 'understand', 'the', 'semantic', 'content', 'of', 'the', 'text', 'in', 'machine', 'translation', 'tokens', 'help', 'align', 'words', 'and', 'phrases', 'across', 'different', 'languages', 'sentiment', 'analysis', 'named', 'entity', 'recognition', 'and', 'information', 'retrieval', 'are', 'other', 'areas', 'where', 'tokenization', 'proves', 'valuable', 'there', 'are', 'several', 'libraries', 'and', 'tools', 'available', 'that', 'offer', 'tokenization', 'functionalities', 'for', 'different', 'programming', 'languages', 'pythonbased', 'libraries', 'like', 'nltk', 'spacy', 'and', 'the', 'hugging', 'face', 'transformers', 'library', 'provide', 'easytouse', 'tokenization', 'methods', 'these', 'libraries', 'often', 'come', 'with', 'pretrained', 'models', 'that', 'can', 'handle', 'tokenization', 'for', 'multiple', 'languages', 'to', 'practice', 'tokenization', 'you', 'can', 'start', 'by', 'selecting', 'a', 'library', 'and', 'exploring', 'its', 'documentation', 'and', 'examples', 'try', 'tokenizing', 'different', 'sentences', 'and', 'texts', 'and', 'observe', 'the', 'resulting', 'tokens', 'experiment', 'with', 'different', 'tokenization', 'options', 'and', 'consider', 'the', 'impact', 'on', 'downstream', 'nlp', 'tasks', 'remember', 'that', 'tokenization', 'is', 'just', 'the', 'first', 'step', 'in', 'nlp', 'pipelines', 'and', 'subsequent', 'processing', 'steps', 'like', 'stemming', 'lemmatization', 'or', 'stop', 'word', 'removal', 'may', 'be', 'necessary', 'depending', 'on', 'the', 'task', 'at', 'hand', 'by', 'practicing', 'tokenization', 'on', 'various', 'texts', 'you', 'can', 'gain', 'a', 'better', 'understanding', 'of', 'how', 'tokens', 'are', 'formed', 'and', 'how', 'they', 'contribute', 'to', 'nlp', 'analysis', 'happy', 'tokenizing', 'i', 'hope', 'this', 'text', 'provides', 'you', 'with', 'ample', 'material', 'to', 'practice', 'tokenization', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'further', 'assistance']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#Its better to preserve punctuation marks for better sentence tokeinzation.\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "words = word_tokenize(text_without_punctuation)\n",
    "\n",
    "print(\"NLTK Sentence Tokenization:\")\n",
    "print(sentences)\n",
    "print(\"\\nNLTK Word Tokenization:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c961d94-9f03-4e47-8964-45880f1244ad",
   "metadata": {},
   "source": [
    "<h2>Sentence tokenization and word tokenization using SpaCy library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e8aeefe-41f0-463f-a30c-df942c80939e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Word Tokenization:\n",
      "['tokenization', 'is', 'a', 'fundamental', 'concept', 'in', 'natural', 'language', 'processing', 'nlp', 'it', 'involves', 'breaking', 'down', 'a', 'piece', 'of', 'text', 'such', 'as', 'a', 'paragraph', 'or', 'a', 'document', 'into', 'smaller', 'units', 'called', 'tokens', 'these', 'tokens', 'can', 'be', 'individual', 'words', 'subwords', 'or', 'even', 'characters', 'tokenization', 'is', 'an', 'essential', 'step', 'in', 'nlp', 'tasks', 'as', 'it', 'provides', 'the', 'foundation', 'for', 'further', 'analysis', 'and', 'processing', 'lets', 'consider', 'an', 'example', 'to', 'understand', 'tokenization', 'better', 'imagine', 'we', 'have', 'the', 'following', 'sentence', 'i', 'love', 'eating', 'pizza', 'when', 'tokenized', 'this', 'sentence', 'might', 'be', 'represented', 'as', 'i', 'love', 'eating', 'pizza', 'here', 'each', 'word', 'is', 'considered', 'a', 'separate', 'token', 'however', 'the', 'tokenization', 'process', 'can', 'be', 'more', 'complex', 'especially', 'for', 'languages', 'with', 'compound', 'words', 'or', 'morphological', 'variations', 'tokenization', 'techniques', 'can', 'vary', 'depending', 'on', 'the', 'requirements', 'of', 'the', 'task', 'and', 'the', 'specific', 'language', 'being', 'processed', 'for', 'instance', 'some', 'languages', 'might', 'employ', 'subword', 'tokenization', 'where', 'words', 'are', 'broken', 'down', 'into', 'smaller', 'units', 'this', 'can', 'help', 'capture', 'morphological', 'information', 'and', 'handle', 'outofvocabulary', 'words', 'more', 'effectively', 'in', 'addition', 'to', 'breaking', 'down', 'text', 'into', 'tokens', 'tokenization', 'also', 'involves', 'handling', 'punctuation', 'special', 'characters', 'and', 'other', 'textual', 'elements', 'for', 'example', 'the', 'sentence', 'hello', 'world', 'might', 'be', 'tokenized', 'into', 'hello', ' ', 'world', ' ', 'where', 'the', 'commas', 'and', 'exclamation', 'mark', 'are', 'treated', 'as', 'separate', 'tokens', 'tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'various', 'nlp', 'applications', 'for', 'text', 'classification', 'tasks', 'tokens', 'serve', 'as', 'input', 'features', 'enabling', 'the', 'model', 'to', 'understand', 'the', 'semantic', 'content', 'of', 'the', 'text', 'in', 'machine', 'translation', 'tokens', 'help', 'align', 'words', 'and', 'phrases', 'across', 'different', 'languages', 'sentiment', 'analysis', 'named', 'entity', 'recognition', 'and', 'information', 'retrieval', 'are', 'other', 'areas', 'where', 'tokenization', 'proves', 'valuable', 'there', 'are', 'several', 'libraries', 'and', 'tools', 'available', 'that', 'offer', 'tokenization', 'functionalities', 'for', 'different', 'programming', 'languages', 'pythonbased', 'libraries', 'like', 'nltk', 'spacy', 'and', 'the', 'hugging', 'face', 'transformers', 'library', 'provide', 'easytouse', 'tokenization', 'methods', 'these', 'libraries', 'often', 'come', 'with', 'pretrained', 'models', 'that', 'can', 'handle', 'tokenization', 'for', 'multiple', 'languages', 'to', 'practice', 'tokenization', 'you', 'can', 'start', 'by', 'selecting', 'a', 'library', 'and', 'exploring', 'its', 'documentation', 'and', 'examples', 'try', 'tokenizing', 'different', 'sentences', 'and', 'texts', 'and', 'observe', 'the', 'resulting', 'tokens', 'experiment', 'with', 'different', 'tokenization', 'options', 'and', 'consider', 'the', 'impact', 'on', 'downstream', 'nlp', 'tasks', 'remember', 'that', 'tokenization', 'is', 'just', 'the', 'first', 'step', 'in', 'nlp', 'pipelines', 'and', 'subsequent', 'processing', 'steps', 'like', 'stemming', 'lemmatization', 'or', 'stop', 'word', 'removal', 'may', 'be', 'necessary', 'depending', 'on', 'the', 'task', 'at', 'hand', 'by', 'practicing', 'tokenization', 'on', 'various', 'texts', 'you', 'can', 'gain', 'a', 'better', 'understanding', 'of', 'how', 'tokens', 'are', 'formed', 'and', 'how', 'they', 'contribute', 'to', 'nlp', 'analysis', 'happy', 'tokenizing', 'i', 'hope', 'this', 'text', 'provides', 'you', 'with', 'ample', 'material', 'to', 'practice', 'tokenization', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'further', 'assistance']\n",
      "spaCy Sentence Tokenization:\n",
      "Tokenization is a fundamental concept in Natural Language Processing (NLP).\n",
      "It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens.\n",
      "These tokens can be individual words, subwords, or even characters.\n",
      "Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing.\n",
      "Let's consider an example to understand tokenization better.\n",
      "Imagine we have the following sentence: 'I love eating pizza.'\n",
      "When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza'].\n",
      "Here, each word is considered a separate token.\n",
      "However, the tokenization process can be more complex, especially for languages with compound words or morphological variations.\n",
      "Tokenization techniques can vary depending on the requirements of the task and the specific language being processed.\n",
      "For instance, some languages might employ subword tokenization, where words are broken down into smaller units.\n",
      "This can help capture morphological information and handle out-of-vocabulary words more effectively.\n",
      "In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements.\n",
      "For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens.\n",
      "Tokenization plays a crucial role in various NLP applications.\n",
      "For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text.\n",
      "In machine translation, tokens help align words and phrases across different languages.\n",
      "Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable.\n",
      "There are several libraries and tools available that offer tokenization functionalities for different programming languages.\n",
      "Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods.\n",
      "These libraries often come with pre-trained models that can handle tokenization for multiple languages.\n",
      "To practice tokenization, you can start by selecting a library and exploring its documentation and examples.\n",
      "Try tokenizing different sentences and texts, and observe the resulting tokens.\n",
      "Experiment with different tokenization options and consider the impact on downstream NLP tasks.\n",
      "Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand.\n",
      "By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis.\n",
      "Happy tokenizing!\"\n",
      "I hope this text provides you with ample material to practice tokenization.\n",
      "Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "#In the spaCy example, the en_core_web_sm model is loaded, which is a small English language model.\n",
    "#The text is then processed using the nlp() function, which returns a spaCy Doc object.\n",
    "#The individual word tokens and sentences tokens are extracted from the document by iterating over the tokens in the doc object.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text_without_punctuation)\n",
    "doc1 = nlp(text)\n",
    "\n",
    "# Extract tokens from the spaCy document\n",
    "tokens = [token.text for token in doc]\n",
    "sentences = list(doc1.sents)\n",
    "\n",
    "print(\"spaCy Word Tokenization:\")\n",
    "print(tokens)\n",
    "print(\"spaCy Sentence Tokenization:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
