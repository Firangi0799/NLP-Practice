{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778abf05-f7f1-4277-92d5-9385151054b3",
   "metadata": {},
   "source": [
    "<h1>POS Tagging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec750c1b-1161-49ee-b451-b65bad025e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tokenization is a fundamental concept in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a paragraph or a document, into smaller units called tokens. These tokens can be individual words, subwords, or even characters. Tokenization is an essential step in NLP tasks as it provides the foundation for further analysis and processing. Let's consider an example to understand tokenization better. Imagine we have the following sentence: 'I love eating pizza.' When tokenized, this sentence might be represented as ['I', 'love', 'eating', 'pizza']. Here, each word is considered a separate token. However, the tokenization process can be more complex, especially for languages with compound words or morphological variations. Tokenization techniques can vary depending on the requirements of the task and the specific language being processed. For instance, some languages might employ subword tokenization, where words are broken down into smaller units. This can help capture morphological information and handle out-of-vocabulary words more effectively. In addition to breaking down text into tokens, tokenization also involves handling punctuation, special characters, and other textual elements. For example, the sentence 'Hello, world!' might be tokenized into ['Hello', ',', 'world', '!'], where the commas and exclamation mark are treated as separate tokens. Tokenization plays a crucial role in various NLP applications. For text classification tasks, tokens serve as input features, enabling the model to understand the semantic content of the text. In machine translation, tokens help align words and phrases across different languages. Sentiment analysis, named entity recognition, and information retrieval are other areas where tokenization proves valuable. There are several libraries and tools available that offer tokenization functionalities for different programming languages. Python-based libraries like NLTK, spaCy, and the Hugging Face Transformers library provide easy-to-use tokenization methods. These libraries often come with pre-trained models that can handle tokenization for multiple languages. To practice tokenization, you can start by selecting a library and exploring its documentation and examples. Try tokenizing different sentences and texts, and observe the resulting tokens. Experiment with different tokenization options and consider the impact on downstream NLP tasks. Remember that tokenization is just the first step in NLP pipelines, and subsequent processing steps like stemming, lemmatization, or stop word removal may be necessary depending on the task at hand. By practicing tokenization on various texts, you can gain a better understanding of how tokens are formed and how they contribute to NLP analysis. Happy tokenizing!\" I hope this text provides you with ample material to practice tokenization. Let me know if you need any further assistance!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea989ca4-e54d-4bf3-a4cd-dfb1fe6123b4",
   "metadata": {},
   "source": [
    "<h2>Remove Punctuation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42001c69-d540-4538-be52-54a4933157de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization is a fundamental concept in natural language processing nlp it involves breaking down a piece of text such as a paragraph or a document into smaller units called tokens these tokens can be individual words subwords or even characters tokenization is an essential step in nlp tasks as it provides the foundation for further analysis and processing lets consider an example to understand tokenization better imagine we have the following sentence i love eating pizza when tokenized this sentence might be represented as i love eating pizza here each word is considered a separate token however the tokenization process can be more complex especially for languages with compound words or morphological variations tokenization techniques can vary depending on the requirements of the task and the specific language being processed for instance some languages might employ subword tokenization where words are broken down into smaller units this can help capture morphological information and handle outofvocabulary words more effectively in addition to breaking down text into tokens tokenization also involves handling punctuation special characters and other textual elements for example the sentence hello world might be tokenized into hello  world  where the commas and exclamation mark are treated as separate tokens tokenization plays a crucial role in various nlp applications for text classification tasks tokens serve as input features enabling the model to understand the semantic content of the text in machine translation tokens help align words and phrases across different languages sentiment analysis named entity recognition and information retrieval are other areas where tokenization proves valuable there are several libraries and tools available that offer tokenization functionalities for different programming languages pythonbased libraries like nltk spacy and the hugging face transformers library provide easytouse tokenization methods these libraries often come with pretrained models that can handle tokenization for multiple languages to practice tokenization you can start by selecting a library and exploring its documentation and examples try tokenizing different sentences and texts and observe the resulting tokens experiment with different tokenization options and consider the impact on downstream nlp tasks remember that tokenization is just the first step in nlp pipelines and subsequent processing steps like stemming lemmatization or stop word removal may be necessary depending on the task at hand by practicing tokenization on various texts you can gain a better understanding of how tokens are formed and how they contribute to nlp analysis happy tokenizing i hope this text provides you with ample material to practice tokenization let me know if you need any further assistance\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text_without_punctuation = text.translate(translator)\n",
    "text_without_punctuation = text_without_punctuation.lower()\n",
    "\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfd60f-cd01-414c-a8d0-3aa2fcb1bfaf",
   "metadata": {},
   "source": [
    "<h2>POS tagging using NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e22a14-5ed7-4be2-b50d-33407f94a6b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tokenization', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('fundamental', 'JJ'), ('concept', 'NN'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'IN'), ('it', 'PRP'), ('involves', 'VBZ'), ('breaking', 'VBG'), ('down', 'RP'), ('a', 'DT'), ('piece', 'NN'), ('of', 'IN'), ('text', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('a', 'DT'), ('paragraph', 'NN'), ('or', 'CC'), ('a', 'DT'), ('document', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('units', 'NNS'), ('called', 'VBD'), ('tokens', 'NNS'), ('these', 'DT'), ('tokens', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('individual', 'JJ'), ('words', 'NNS'), ('subwords', 'NNS'), ('or', 'CC'), ('even', 'RB'), ('characters', 'NNS'), ('tokenization', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('essential', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('nlp', 'JJ'), ('tasks', 'NNS'), ('as', 'IN'), ('it', 'PRP'), ('provides', 'VBZ'), ('the', 'DT'), ('foundation', 'NN'), ('for', 'IN'), ('further', 'JJ'), ('analysis', 'NN'), ('and', 'CC'), ('processing', 'NN'), ('lets', 'NNS'), ('consider', 'VBP'), ('an', 'DT'), ('example', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('tokenization', 'NN'), ('better', 'RB'), ('imagine', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('the', 'DT'), ('following', 'JJ'), ('sentence', 'NN'), ('i', 'JJ'), ('love', 'VBP'), ('eating', 'VBG'), ('pizza', 'NN'), ('when', 'WRB'), ('tokenized', 'VBN'), ('this', 'DT'), ('sentence', 'NN'), ('might', 'MD'), ('be', 'VB'), ('represented', 'VBN'), ('as', 'IN'), ('i', 'JJ'), ('love', 'VBP'), ('eating', 'VBG'), ('pizza', 'NN'), ('here', 'RB'), ('each', 'DT'), ('word', 'NN'), ('is', 'VBZ'), ('considered', 'VBN'), ('a', 'DT'), ('separate', 'JJ'), ('token', 'NN'), ('however', 'RB'), ('the', 'DT'), ('tokenization', 'NN'), ('process', 'NN'), ('can', 'MD'), ('be', 'VB'), ('more', 'RBR'), ('complex', 'JJ'), ('especially', 'RB'), ('for', 'IN'), ('languages', 'NNS'), ('with', 'IN'), ('compound', 'NN'), ('words', 'NNS'), ('or', 'CC'), ('morphological', 'JJ'), ('variations', 'NNS'), ('tokenization', 'NN'), ('techniques', 'NNS'), ('can', 'MD'), ('vary', 'VB'), ('depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('requirements', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('task', 'NN'), ('and', 'CC'), ('the', 'DT'), ('specific', 'JJ'), ('language', 'NN'), ('being', 'VBG'), ('processed', 'VBN'), ('for', 'IN'), ('instance', 'NN'), ('some', 'DT'), ('languages', 'NNS'), ('might', 'MD'), ('employ', 'VB'), ('subword', 'NN'), ('tokenization', 'NN'), ('where', 'WRB'), ('words', 'NNS'), ('are', 'VBP'), ('broken', 'VBN'), ('down', 'RP'), ('into', 'IN'), ('smaller', 'JJR'), ('units', 'NNS'), ('this', 'DT'), ('can', 'MD'), ('help', 'VB'), ('capture', 'VB'), ('morphological', 'JJ'), ('information', 'NN'), ('and', 'CC'), ('handle', 'NN'), ('outofvocabulary', 'JJ'), ('words', 'NNS'), ('more', 'RBR'), ('effectively', 'RB'), ('in', 'IN'), ('addition', 'NN'), ('to', 'TO'), ('breaking', 'VBG'), ('down', 'RP'), ('text', 'RB'), ('into', 'IN'), ('tokens', 'NNS'), ('tokenization', 'NN'), ('also', 'RB'), ('involves', 'VBZ'), ('handling', 'VBG'), ('punctuation', 'NN'), ('special', 'JJ'), ('characters', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('textual', 'JJ'), ('elements', 'NNS'), ('for', 'IN'), ('example', 'NN'), ('the', 'DT'), ('sentence', 'NN'), ('hello', 'NN'), ('world', 'NN'), ('might', 'MD'), ('be', 'VB'), ('tokenized', 'VBN'), ('into', 'IN'), ('hello', 'JJ'), ('world', 'NN'), ('where', 'WRB'), ('the', 'DT'), ('commas', 'NN'), ('and', 'CC'), ('exclamation', 'NN'), ('mark', 'NN'), ('are', 'VBP'), ('treated', 'VBN'), ('as', 'IN'), ('separate', 'JJ'), ('tokens', 'NNS'), ('tokenization', 'NN'), ('plays', 'VBZ'), ('a', 'DT'), ('crucial', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('various', 'JJ'), ('nlp', 'JJ'), ('applications', 'NNS'), ('for', 'IN'), ('text', 'JJ'), ('classification', 'NN'), ('tasks', 'NNS'), ('tokens', 'VBZ'), ('serve', 'VBP'), ('as', 'IN'), ('input', 'NN'), ('features', 'NNS'), ('enabling', 'VBG'), ('the', 'DT'), ('model', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('semantic', 'JJ'), ('content', 'NN'), ('of', 'IN'), ('the', 'DT'), ('text', 'NN'), ('in', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('tokens', 'VBZ'), ('help', 'VBP'), ('align', 'VB'), ('words', 'NNS'), ('and', 'CC'), ('phrases', 'NNS'), ('across', 'IN'), ('different', 'JJ'), ('languages', 'NNS'), ('sentiment', 'NN'), ('analysis', 'NN'), ('named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('and', 'CC'), ('information', 'NN'), ('retrieval', 'NN'), ('are', 'VBP'), ('other', 'JJ'), ('areas', 'NNS'), ('where', 'WRB'), ('tokenization', 'NN'), ('proves', 'VBZ'), ('valuable', 'JJ'), ('there', 'EX'), ('are', 'VBP'), ('several', 'JJ'), ('libraries', 'NNS'), ('and', 'CC'), ('tools', 'NNS'), ('available', 'JJ'), ('that', 'WDT'), ('offer', 'VBP'), ('tokenization', 'NN'), ('functionalities', 'NNS'), ('for', 'IN'), ('different', 'JJ'), ('programming', 'NN'), ('languages', 'NNS'), ('pythonbased', 'VBD'), ('libraries', 'NNS'), ('like', 'IN'), ('nltk', 'JJ'), ('spacy', 'NN'), ('and', 'CC'), ('the', 'DT'), ('hugging', 'NN'), ('face', 'NN'), ('transformers', 'NNS'), ('library', 'JJ'), ('provide', 'VBP'), ('easytouse', 'IN'), ('tokenization', 'NN'), ('methods', 'NNS'), ('these', 'DT'), ('libraries', 'NNS'), ('often', 'RB'), ('come', 'VBP'), ('with', 'IN'), ('pretrained', 'JJ'), ('models', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('handle', 'VB'), ('tokenization', 'NN'), ('for', 'IN'), ('multiple', 'JJ'), ('languages', 'NNS'), ('to', 'TO'), ('practice', 'NN'), ('tokenization', 'NN'), ('you', 'PRP'), ('can', 'MD'), ('start', 'VB'), ('by', 'IN'), ('selecting', 'VBG'), ('a', 'DT'), ('library', 'NN'), ('and', 'CC'), ('exploring', 'VBG'), ('its', 'PRP$'), ('documentation', 'NN'), ('and', 'CC'), ('examples', 'NNS'), ('try', 'VBP'), ('tokenizing', 'VBG'), ('different', 'JJ'), ('sentences', 'NNS'), ('and', 'CC'), ('texts', 'NNS'), ('and', 'CC'), ('observe', 'VBP'), ('the', 'DT'), ('resulting', 'VBG'), ('tokens', 'NNS'), ('experiment', 'NN'), ('with', 'IN'), ('different', 'JJ'), ('tokenization', 'NN'), ('options', 'NNS'), ('and', 'CC'), ('consider', 'VB'), ('the', 'DT'), ('impact', 'NN'), ('on', 'IN'), ('downstream', 'NN'), ('nlp', 'NN'), ('tasks', 'NNS'), ('remember', 'VBP'), ('that', 'IN'), ('tokenization', 'NN'), ('is', 'VBZ'), ('just', 'RB'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('nlp', 'JJ'), ('pipelines', 'NNS'), ('and', 'CC'), ('subsequent', 'JJ'), ('processing', 'NN'), ('steps', 'NNS'), ('like', 'IN'), ('stemming', 'VBG'), ('lemmatization', 'NN'), ('or', 'CC'), ('stop', 'VB'), ('word', 'NN'), ('removal', 'NN'), ('may', 'MD'), ('be', 'VB'), ('necessary', 'JJ'), ('depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('task', 'NN'), ('at', 'IN'), ('hand', 'NN'), ('by', 'IN'), ('practicing', 'VBG'), ('tokenization', 'NN'), ('on', 'IN'), ('various', 'JJ'), ('texts', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('gain', 'VB'), ('a', 'DT'), ('better', 'JJR'), ('understanding', 'NN'), ('of', 'IN'), ('how', 'WRB'), ('tokens', 'NNS'), ('are', 'VBP'), ('formed', 'VBN'), ('and', 'CC'), ('how', 'WRB'), ('they', 'PRP'), ('contribute', 'VBP'), ('to', 'TO'), ('nlp', 'VB'), ('analysis', 'NN'), ('happy', 'JJ'), ('tokenizing', 'NN'), ('i', 'NN'), ('hope', 'VBP'), ('this', 'DT'), ('text', 'NN'), ('provides', 'VBZ'), ('you', 'PRP'), ('with', 'IN'), ('ample', 'JJ'), ('material', 'NN'), ('to', 'TO'), ('practice', 'NN'), ('tokenization', 'NN'), ('let', 'VB'), ('me', 'PRP'), ('know', 'VB'), ('if', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('any', 'DT'), ('further', 'JJ'), ('assistance', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(text_without_punctuation)\n",
    "\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb3aec-67ba-4b68-a4a7-d6aa15edbcb5",
   "metadata": {},
   "source": [
    "<h2>POS tagging using NLTK and ML</h2>\n",
    "<h3>If you're using a machine learning-based approach and have a labeled training dataset, you can train a custom POS tagger. Here's a simple example using NLTK's Hidden Markov Model (HMM) tagger:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a58241c5-344a-40b3-a58a-886b5908a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('test', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "training_data = [\n",
    "    (\"This\", \"DT\"), (\"is\", \"VBZ\"), (\"an\", \"DT\"), (\"example\", \"NN\"), (\"sentence\", \"NN\"),\n",
    "    (\"for\", \"IN\"), (\"tagging\", \"NN\"), (\".\", \".\")\n",
    "]\n",
    "\n",
    "tagger = nltk.tag.hmm.HiddenMarkovModelTagger.train([training_data])\n",
    "\n",
    "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\", \".\"]\n",
    "tagged_sentence = tagger.tag(test_sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217da63-991c-4cdf-b48c-3e956289be32",
   "metadata": {},
   "source": [
    "<h2>POS tagging using SpaCy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "823b9b90-d5e7-4b5b-9f7b-2c3818add374",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tokenization', 'NOUN'), ('is', 'AUX'), ('a', 'DET'), ('fundamental', 'ADJ'), ('concept', 'NOUN'), ('in', 'ADP'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('nlp', 'NOUN'), ('it', 'PRON'), ('involves', 'VERB'), ('breaking', 'VERB'), ('down', 'ADP'), ('a', 'DET'), ('piece', 'NOUN'), ('of', 'ADP'), ('text', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('a', 'DET'), ('paragraph', 'NOUN'), ('or', 'CCONJ'), ('a', 'DET'), ('document', 'NOUN'), ('into', 'ADP'), ('smaller', 'ADJ'), ('units', 'NOUN'), ('called', 'VERB'), ('tokens', 'NOUN'), ('these', 'DET'), ('tokens', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('individual', 'ADJ'), ('words', 'NOUN'), ('subwords', 'NOUN'), ('or', 'CCONJ'), ('even', 'ADV'), ('characters', 'NOUN'), ('tokenization', 'NOUN'), ('is', 'AUX'), ('an', 'DET'), ('essential', 'ADJ'), ('step', 'NOUN'), ('in', 'ADP'), ('nlp', 'ADJ'), ('tasks', 'NOUN'), ('as', 'SCONJ'), ('it', 'PRON'), ('provides', 'VERB'), ('the', 'DET'), ('foundation', 'NOUN'), ('for', 'ADP'), ('further', 'ADJ'), ('analysis', 'NOUN'), ('and', 'CCONJ'), ('processing', 'NOUN'), ('lets', 'NOUN'), ('consider', 'VERB'), ('an', 'DET'), ('example', 'NOUN'), ('to', 'PART'), ('understand', 'VERB'), ('tokenization', 'NOUN'), ('better', 'ADV'), ('imagine', 'VERB'), ('we', 'PRON'), ('have', 'VERB'), ('the', 'DET'), ('following', 'ADJ'), ('sentence', 'NOUN'), ('i', 'PRON'), ('love', 'VERB'), ('eating', 'VERB'), ('pizza', 'NOUN'), ('when', 'SCONJ'), ('tokenized', 'VERB'), ('this', 'DET'), ('sentence', 'NOUN'), ('might', 'AUX'), ('be', 'AUX'), ('represented', 'VERB'), ('as', 'SCONJ'), ('i', 'PRON'), ('love', 'VERB'), ('eating', 'VERB'), ('pizza', 'NOUN'), ('here', 'ADV'), ('each', 'DET'), ('word', 'NOUN'), ('is', 'AUX'), ('considered', 'VERB'), ('a', 'DET'), ('separate', 'ADJ'), ('token', 'NOUN'), ('however', 'ADV'), ('the', 'DET'), ('tokenization', 'NOUN'), ('process', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('more', 'ADV'), ('complex', 'ADJ'), ('especially', 'ADV'), ('for', 'ADP'), ('languages', 'NOUN'), ('with', 'ADP'), ('compound', 'ADJ'), ('words', 'NOUN'), ('or', 'CCONJ'), ('morphological', 'ADJ'), ('variations', 'NOUN'), ('tokenization', 'NOUN'), ('techniques', 'NOUN'), ('can', 'AUX'), ('vary', 'VERB'), ('depending', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('requirements', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('task', 'NOUN'), ('and', 'CCONJ'), ('the', 'DET'), ('specific', 'ADJ'), ('language', 'NOUN'), ('being', 'AUX'), ('processed', 'VERB'), ('for', 'ADP'), ('instance', 'NOUN'), ('some', 'DET'), ('languages', 'NOUN'), ('might', 'AUX'), ('employ', 'VERB'), ('subword', 'PROPN'), ('tokenization', 'PROPN'), ('where', 'SCONJ'), ('words', 'NOUN'), ('are', 'AUX'), ('broken', 'VERB'), ('down', 'ADP'), ('into', 'ADP'), ('smaller', 'ADJ'), ('units', 'NOUN'), ('this', 'PRON'), ('can', 'AUX'), ('help', 'VERB'), ('capture', 'VERB'), ('morphological', 'ADJ'), ('information', 'NOUN'), ('and', 'CCONJ'), ('handle', 'VERB'), ('outofvocabulary', 'ADJ'), ('words', 'NOUN'), ('more', 'ADV'), ('effectively', 'ADV'), ('in', 'ADP'), ('addition', 'NOUN'), ('to', 'ADP'), ('breaking', 'VERB'), ('down', 'ADP'), ('text', 'NOUN'), ('into', 'ADP'), ('tokens', 'PROPN'), ('tokenization', 'NOUN'), ('also', 'ADV'), ('involves', 'VERB'), ('handling', 'VERB'), ('punctuation', 'NOUN'), ('special', 'ADJ'), ('characters', 'NOUN'), ('and', 'CCONJ'), ('other', 'ADJ'), ('textual', 'ADJ'), ('elements', 'NOUN'), ('for', 'ADP'), ('example', 'NOUN'), ('the', 'DET'), ('sentence', 'NOUN'), ('hello', 'PROPN'), ('world', 'NOUN'), ('might', 'AUX'), ('be', 'AUX'), ('tokenized', 'VERB'), ('into', 'ADP'), ('hello', 'PROPN'), (' ', 'SPACE'), ('world', 'NOUN'), (' ', 'SPACE'), ('where', 'SCONJ'), ('the', 'DET'), ('commas', 'PROPN'), ('and', 'CCONJ'), ('exclamation', 'NOUN'), ('mark', 'NOUN'), ('are', 'AUX'), ('treated', 'VERB'), ('as', 'SCONJ'), ('separate', 'ADJ'), ('tokens', 'NOUN'), ('tokenization', 'NOUN'), ('plays', 'VERB'), ('a', 'DET'), ('crucial', 'ADJ'), ('role', 'NOUN'), ('in', 'ADP'), ('various', 'ADJ'), ('nlp', 'NOUN'), ('applications', 'NOUN'), ('for', 'ADP'), ('text', 'NOUN'), ('classification', 'NOUN'), ('tasks', 'NOUN'), ('tokens', 'NOUN'), ('serve', 'VERB'), ('as', 'SCONJ'), ('input', 'NOUN'), ('features', 'NOUN'), ('enabling', 'VERB'), ('the', 'DET'), ('model', 'NOUN'), ('to', 'PART'), ('understand', 'VERB'), ('the', 'DET'), ('semantic', 'ADJ'), ('content', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('text', 'NOUN'), ('in', 'ADP'), ('machine', 'NOUN'), ('translation', 'NOUN'), ('tokens', 'NOUN'), ('help', 'VERB'), ('align', 'VERB'), ('words', 'NOUN'), ('and', 'CCONJ'), ('phrases', 'NOUN'), ('across', 'ADP'), ('different', 'ADJ'), ('languages', 'NOUN'), ('sentiment', 'NOUN'), ('analysis', 'NOUN'), ('named', 'VERB'), ('entity', 'NOUN'), ('recognition', 'NOUN'), ('and', 'CCONJ'), ('information', 'NOUN'), ('retrieval', 'NOUN'), ('are', 'AUX'), ('other', 'ADJ'), ('areas', 'NOUN'), ('where', 'SCONJ'), ('tokenization', 'NOUN'), ('proves', 'VERB'), ('valuable', 'ADJ'), ('there', 'PRON'), ('are', 'VERB'), ('several', 'ADJ'), ('libraries', 'NOUN'), ('and', 'CCONJ'), ('tools', 'NOUN'), ('available', 'ADJ'), ('that', 'PRON'), ('offer', 'VERB'), ('tokenization', 'NOUN'), ('functionalities', 'NOUN'), ('for', 'ADP'), ('different', 'ADJ'), ('programming', 'NOUN'), ('languages', 'NOUN'), ('pythonbased', 'VERB'), ('libraries', 'NOUN'), ('like', 'ADP'), ('nltk', 'PROPN'), ('spacy', 'NOUN'), ('and', 'CCONJ'), ('the', 'DET'), ('hugging', 'VERB'), ('face', 'NOUN'), ('transformers', 'NOUN'), ('library', 'NOUN'), ('provide', 'VERB'), ('easytouse', 'NOUN'), ('tokenization', 'NOUN'), ('methods', 'NOUN'), ('these', 'DET'), ('libraries', 'NOUN'), ('often', 'ADV'), ('come', 'VERB'), ('with', 'ADP'), ('pretrained', 'VERB'), ('models', 'NOUN'), ('that', 'PRON'), ('can', 'AUX'), ('handle', 'VERB'), ('tokenization', 'NOUN'), ('for', 'ADP'), ('multiple', 'ADJ'), ('languages', 'NOUN'), ('to', 'PART'), ('practice', 'VERB'), ('tokenization', 'NOUN'), ('you', 'PRON'), ('can', 'AUX'), ('start', 'VERB'), ('by', 'ADP'), ('selecting', 'VERB'), ('a', 'DET'), ('library', 'NOUN'), ('and', 'CCONJ'), ('exploring', 'VERB'), ('its', 'PRON'), ('documentation', 'NOUN'), ('and', 'CCONJ'), ('examples', 'NOUN'), ('try', 'AUX'), ('tokenizing', 'VERB'), ('different', 'ADJ'), ('sentences', 'NOUN'), ('and', 'CCONJ'), ('texts', 'NOUN'), ('and', 'CCONJ'), ('observe', 'VERB'), ('the', 'DET'), ('resulting', 'VERB'), ('tokens', 'NOUN'), ('experiment', 'NOUN'), ('with', 'ADP'), ('different', 'ADJ'), ('tokenization', 'NOUN'), ('options', 'NOUN'), ('and', 'CCONJ'), ('consider', 'VERB'), ('the', 'DET'), ('impact', 'NOUN'), ('on', 'ADP'), ('downstream', 'ADJ'), ('nlp', 'NOUN'), ('tasks', 'NOUN'), ('remember', 'VERB'), ('that', 'SCONJ'), ('tokenization', 'NOUN'), ('is', 'AUX'), ('just', 'ADV'), ('the', 'DET'), ('first', 'ADJ'), ('step', 'NOUN'), ('in', 'ADP'), ('nlp', 'NOUN'), ('pipelines', 'NOUN'), ('and', 'CCONJ'), ('subsequent', 'ADJ'), ('processing', 'NOUN'), ('steps', 'NOUN'), ('like', 'ADP'), ('stemming', 'VERB'), ('lemmatization', 'NOUN'), ('or', 'CCONJ'), ('stop', 'VERB'), ('word', 'NOUN'), ('removal', 'NOUN'), ('may', 'AUX'), ('be', 'AUX'), ('necessary', 'ADJ'), ('depending', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('task', 'NOUN'), ('at', 'ADP'), ('hand', 'NOUN'), ('by', 'ADP'), ('practicing', 'VERB'), ('tokenization', 'NOUN'), ('on', 'ADP'), ('various', 'ADJ'), ('texts', 'NOUN'), ('you', 'PRON'), ('can', 'AUX'), ('gain', 'VERB'), ('a', 'DET'), ('better', 'ADJ'), ('understanding', 'NOUN'), ('of', 'ADP'), ('how', 'SCONJ'), ('tokens', 'NOUN'), ('are', 'AUX'), ('formed', 'VERB'), ('and', 'CCONJ'), ('how', 'SCONJ'), ('they', 'PRON'), ('contribute', 'VERB'), ('to', 'PART'), ('nlp', 'VERB'), ('analysis', 'NOUN'), ('happy', 'ADJ'), ('tokenizing', 'NOUN'), ('i', 'PRON'), ('hope', 'VERB'), ('this', 'DET'), ('text', 'NOUN'), ('provides', 'VERB'), ('you', 'PRON'), ('with', 'ADP'), ('ample', 'ADJ'), ('material', 'NOUN'), ('to', 'PART'), ('practice', 'VERB'), ('tokenization', 'NOUN'), ('let', 'VERB'), ('me', 'PRON'), ('know', 'VERB'), ('if', 'SCONJ'), ('you', 'PRON'), ('need', 'VERB'), ('any', 'DET'), ('further', 'ADJ'), ('assistance', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    tagged_words = [(token.text, token.pos_) for token in doc]\n",
    "    return tagged_words\n",
    "\n",
    "tagged_sentence = pos_tagging(text_without_punctuation)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878691bc-2afd-4bce-93a0-de3f64c50588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
